# NLP-ENSAE-3A-Sujet4
This project is part of the 3rd year NLP course at ENSAE. We have chosen to work on subject 4 namely : Text Similarity as An Evaluation Measure of Text Generation

The aim of this project is to benchmark the correlation of existing metrics of NLG evaluation with human scores over time.
We decided to work on story generation. 

Our work is thus based on the following paper : 
Cyril Chhun, Pierre Colombo, Fabian Suchanek, Chloe Clavel Of Human Criteria and Automatic Metrics: A Benchmark of the Evaluation of Story Generation (oral) COLING 2022

You can find two notebooks:
- data_visualization.ipynb with the HANNA database 
- ROCStories dataset.ipynb with the ROCStories database

## Acknowledgments

- We used the files 'hanna_stories_annotation.csv' and 'hanna_metric_scores.csv' from the https://github.com/dig-team/hanna-benchmark-asg
- We cloned the https://github.com/thu-coai/OpenMEVA and copied their test.py function

