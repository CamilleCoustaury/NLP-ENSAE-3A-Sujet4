# NLP-ENSAE-3A-Sujet4
This project is part of the 3rd year NLP course at ENSAE. We have chosen to work on subject 4 namely : Text Similarity as An Evaluation Measure of Text Generation

The aim of this project is to benchmark the correlation of existing metrics of NLG evaluation with human scores.
We decided to work on story generation. 

Our work is thus based on the following paper : 
Cyril Chhun, Pierre Colombo, Fabian Suchanek, Chloe Clavel Of Human Criteria and Automatic Metrics: A Benchmark of the Evaluation of Story Generation (oral) COLING 2022

**STEP 1 :**
Find NLG data. 
We use the following repository to access the data : 
https://github.com/dig-team/hanna-benchmark-asg

**STEP 2 :**
Compute the metrics on the generated stories. 

**STEP 3 :**
Compute the correlation between metrics and human score 


## Acknowledgments

- We used the files 'hanna_stories_annotation.csv' and 'hanna_metric_scores.csv' from the https://github.com/dig-team/hanna-benchmark-asg
- We cloned the https://github.com/thu-coai/OpenMEVA and copied their test.py function

